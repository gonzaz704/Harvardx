---
title: "Boston Housing report"
author: "Gonzalo Vazquez"
date: "2023-03-17"
output: pdf_document
---



## Introduction
The "Boston Housing" dataset contains information collected by the U.S Census Service concerning housing in the area of Boston, Massachusetts. The dataset has 506 rows and 14 columns, with each row representing a suburb of Boston. The goal of this project is to predict the median value of owner-occupied homes in thousands of dollars (medv) based on 13 other attributes such as crime rate, number of rooms, and accessibility to highways.

The key steps that will be performed include:

Data cleaning: Handling missing values and scaling the data
Data exploration and visualization: Displaying summary statistics of the dataset, as well as visualizing the relationship between the variables.
Model selection and training: We will use four different models: Linear regression, Random Forest, Ridge Regression and "Feature-selected model" custom by myself, with the aim of predicting the median value of owner-occupied homes (medv)
Model evaluation: We will compare the performance of the four models using RMSE.
\



## Exploring the dataset 

```{r loading data, echo=FALSE, message=FALSE, warning=FALSE}
#Loading Data

# Loading/installing packages
if(!require(randomForest)) install.packages("randomForest")
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(knitr)) install.packages("knitr")
if(!require(kableExtra)) install.packages("kableExtra")
if(!require(glmnet)) install.packages("glmnet")
if(!require(corrplot)) install.packages("corrplot")


# We first load the "Boston Housing" dataset into R. It could be loaded from the library or uncomment the url to load the dataset directly from the .csv file 
library(randomForest)
library(MASS)
data(Boston)
# Boston <- read.csv("https://raw.githubusercontent.com/gonzaz704/Edx/main/HousingData.csv")
```

First I will observe some of the rows and columns of the dataset to get a general idea of their content
```{r head, echo=FALSE}
# View the first few rows of the dataset
kable(head(Boston))
```
\

\
I will show here the full names and descriptions of each variable in the dataset, this is from Boston Housing's documentation
```{r, Display the variable names and descriptions, echo=FALSE}
# Display the variable names and descriptions

# Define a data frame with variable names and descriptions
vars <- c("crim", "zn", "indus", "chas", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv")
desc <- c("per capita crime rate by town", "proportion of residential land zoned for lots over 25,000 sq.ft.", "proportion of non-retail business acres per town", "Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)", "nitrogen oxides concentration (parts per 10 million)", "average number of rooms per dwelling", "proportion of owner-occupied units built prior to 1940", "weighted distances to five Boston employment centres", "index of accessibility to radial highways", "full-value property-tax rate per $10,000", "pupil-teacher ratio by town", "1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town", "lower status of the population (percent)", "median value of owner-occupied homes in $1000s")
var_desc_df <- data.frame(variable = vars, description = desc)

# Create a formatted table using kable and kableExtra
kable(var_desc_df, format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = "striped")
```
\
\
\
\
\
\
\
\
Then I will split the dataset into 70% for training and 30% for testing. The reason is because Boston Housting dataset it is not too large so a common rule of thumb is to use a split ratio of 70/30 or 80/20 for smaller datasets, while larger datasets may use a split ratio of 90/10 or even 95/5. The reason for this is that a larger training set may help to improve the performance of more complex models, but at the same time, a smaller testing set may lead to higher variance in the evaluation of the model's performance. In this case using a split ratio of 70/30 can provide a balance between having enough data for training the model while still having enough data for testing and evaluation.

```{r Splitting train/test set, echo=TRUE}
# Splitting train/test set
set.seed(123)
train_index <- sample(nrow(Boston), floor(0.7*nrow(Boston)))
Boston_train <- Boston[train_index,]
Boston_test <- Boston[-train_index,]
```
\




\
Here we do data cleaning so I will count the number of missing values in the dataset
```{r Handling Missing Values, echo=FALSE}
# Handling Missing Values
colSums(is.na(Boston))
``` 
\


\
\
\
\
I will also visualize the distribution of each variable. 
\
```{r Boxplot, echo=FALSE}
# Boxplot
boxplot(Boston, las = 2)
```

\



Here I will scale the variables so all of them are on the same scale and have equal importance in the analysis
```{r Boston_scaled, echo=FALSE}
# Scaling the Data
Boston_scaled <- as.data.frame(scale(Boston_train))
summary(Boston_scaled)
```
\





Another visual representation is about the relationship between number of rooms and median value of owner-occupied homes in Boston 
\
```{r Plot, echo=FALSE}
# Scatter plot of the 'rm' and 'medv' variables in the 'Boston_scaled' dataset
ggplot(Boston_scaled, aes(x = rm, y = medv)) + 
  geom_point() + 
  ggtitle("Relationship between Number of Rooms and Median Value of 
          Owner-Occupied Homes in thousands of dollars")

```
\


We can see that there is a positive correlation between the two variables - as the number of rooms increases, so does the median value of the homes. 
Since the data has been standardized using the scale() function, the mean of each variable is 0. Any value below 0 indicates that the original value was lower than the mean, and any value above 0 indicates that the original value was higher than the mean.

For example, if the "rm" variable has a value of -1, it means that the number of rooms in that particular observation is 1 standard deviation below the mean number of rooms in the "Boston_train" data frame. Similarly, if the "medv" variable has a value of -2, it means that the median value of owner-occupied homes in that particular observation is 2 standard deviations below the mean value in the "Boston_train" data frame.




## Analysis and modeling approach

I will use four different models to predict the median value of owner-occupied homes (medv) based on the 13 predictor variables in the dataset. The models we will use are:

* Linear regression
* Random Forest
* Ridge regression
* Feature-selection

Before building the models, let's first take a look at the correlation matrix of the predictor variables to see which ones are strongly correlated with the response variable medv.
\
```{r Correlation, echo=FALSE}
# Correlation Matrix
corr <- cor(Boston_scaled[, -ncol(Boston_scaled)])
corrplot::corrplot(corr, method = "color", type = "upper", order = "hclust")

```
\

\

```{r Correlation matrix with medv, echo=FALSE}

# compute correlation matrix between all variables
corr_matrix <- cor(Boston_scaled)

# select the column of correlations with "medv"
medv_correlations <- corr_matrix[, "medv"]

# view the correlation values
print(medv_correlations)
```
\
From the correlation matrix, we can see that the variables with the strongest positive correlation with medv are rm (the average number of rooms per dwelling) and zn (the proportion of residential land zoned for lots over 25,000 sq.ft.). The variables with the strongest negative correlation with medv are lstat (the percentage of lower status of the population) and ptratio (the pupil-teacher ratio by town).



## Linear regression 
Linear regression is a simple and commonly used method for predicting numerical values. It assumes a linear relationship between the independent variables and the dependent variable. In the context of the Boston Housing dataset, linear regression can be used to build a model that predicts the median value of owner-occupied homes based on the other features.
\
```{r Linear regression, echo=FALSE}
# Linear Regression Model
model_lm <- lm(medv ~ ., data = Boston_train)
summary(model_lm)
rmse_lm <- sqrt(mean((predict(model_lm, Boston_test) - Boston_test$medv)^2))
```
\
The summary of the linear regression model shows that the variables with the highest coefficient estimates are rm, lstat, and ptratio, which aligns with what we saw in the correlation matrix. However, we also see that some variables, such as chas, indus, and age, have coefficients that are not statistically significant, which means they may not have a strong relationship with medv.
\
```{r Result Linear regression, echo=FALSE}
# Result Linear regression Model
results_table <- data.frame(Method = "Linear regression model", RMSE = rmse_lm)
kable(results_table, align = "c", col.names = c("Method", "RMSE"))
```


## Random Forest
Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to produce a more accurate result. Random Forests are effective in handling complex and high-dimensional data, which makes them a good choice for the Boston Housing dataset, which has multiple features.
```{r Random Forest Model, echo=FALSE}
# Random Forest Model
model_rf <- randomForest(medv ~ ., data = Boston_train, ntree = 500, importance = TRUE)
print(model_rf)
```

The random forest model provides a more accurate prediction of medv, with an out-of-bag (OOB) error rate of 7.07%. We can also see from the variable importance plot that rm and lstat are the two most important variables in predicting medv.

\
```{r Variable Importance, echo=FALSE}
# Variable Importance
varImpPlot(model_rf, type = 1, main = "Variable Importance in Random Forest Model")
```
\

```{r Result Random Forest, echo=FALSE}
# Result Random Forest Model
rmse_rf <- sqrt(mean((predict(model_rf, Boston_test) - Boston_test$medv)^2))
results_table <- data.frame(Method = "Random Forest model", RMSE = rmse_rf)
kable(results_table, align = "c", col.names = c("Method", "RMSE"))

```


## Ridge regression
Ridge regression is a regularized regression method that is used to prevent overfitting in a linear regression model. It adds a penalty term to the sum of squared errors, which reduces the magnitude of the coefficients and leads to a simpler model.
\
```{r Ridge Regression Model, echo=FALSE}
# Fitting a Ridge Regression Model using cross-validation
model_ridge <- cv.glmnet(x = as.matrix(Boston_train[, -ncol(Boston_train)]), y = Boston_train$medv, alpha = 0, nfolds = 10, standardize = TRUE)

# Plotting the cross-validation results
plot(model_ridge, main = '')

# Showing optimal value of lambda (the tuning parameter that controls the amount of regularization)
model_ridge$lambda.min

# Showing the coefficients of the model at the optimal value of lambda
coef(model_ridge, s = model_ridge$lambda.min)
```
\
The ridge regression model applied to the Boston Housing dataset shows that the variables with the highest coefficient estimates are "nox", "rm", and "chas", indicating a strong relationship with medv. However, the coefficients of the other variables have been shrunk towards zero due to the regularization penalty, which helps to prevent overfitting but makes it harder to interpret their effect on medv. Therefore, while these variables may still have a relationship with medv, their effect is less pronounced compared to the variables with higher coefficients.

```{r Result Ridge Regression, echo=FALSE}
# Result Ridge Regression Model
rmse_ridge <- sqrt(mean((predict(model_ridge, as.matrix(Boston_test[, -ncol(Boston_test)])) - Boston_test$medv)^2))
results_table <- data.frame(Method = "Ridge Regression model", RMSE = rmse_ridge)
kable(results_table, align = "c", col.names = c("Method", "RMSE"))

```




## Feature-selected model
We can use the information from the three models and combine them into a single model that takes advantage of the strengths of each approach.

First, we can use the linear regression model to identify the most important variables, which are rm, lstat, and ptratio. These variables have the highest coefficient estimates and are also highlighted as important by the random forest model. We can then use ridge regression to build a regularized linear regression model that includes only these variables, which will help prevent overfitting.
\
```{r Combination, echo=FALSE}
#This code builds the linear regression model using all the variables in the training set. It then identifies the three most important variables based on the results of the linear regression and random forest models. Next, it builds a ridge regression model using only these three variables and selects the optimal regularization parameter using cross-validation. Finally, it makes predictions on the test data and calculates the RMSE

# Build the linear regression model 
model_lm <- lm(medv ~ ., data = Boston_train)

# Identify the most important variables
important_vars <- c("rm", "lstat", "ptratio")

# Build the ridge regression model using only the important variables
X_train <- as.matrix(Boston_train[, important_vars])
y_train <- Boston_train$medv
X_test <- as.matrix(Boston_test[, important_vars])

# Use cross-validation to select the optimal regularization parameter
model_ridge2 <- cv.glmnet(X_train, y_train, alpha = 0, nfolds = 10, standardize = TRUE)

# Make predictions on the test data
y_pred <- predict(model_ridge2, s = model_ridge$lambda.min, newx = X_test)

# Calculate the RMSE
RMSE <- sqrt(mean((y_pred - Boston_test$medv)^2))

results_table <- data.frame(Method = "Feature-selected model", RMSE = RMSE)
kable(results_table, align = "c", col.names = c("Method", "RMSE"))
```
\




## Final results
Model Evaluation.
To evaluate the performance of the fourth models, we will use the root mean squared error (RMSE) metric, which measures the difference between the predicted values and the actual values of medv. Lower values of RMSE indicate better performance.
\
```{r RMSE, echo=FALSE}
# RMSE results
Method <- c("Ridge regression model", "Feature-selected model", "Linear regression model", "Random forest model")
RMSE <- c("5.228", "5.092", "4.802", "3.343")

df <- data.frame(Method, RMSE)
knitr::kable(df, align = "c")

```
\
The results show that the Random Forest algorithm has the lowest RMSE value,  with an RMSE of 3.34 compared to 4.80 for the linear regression model, 5.22 for Ridge Regression model and 5.09 of the Feature-selected model, indicating that Random Forest is the most accurate algorithm for predicting housing values in Boston suburbs.





## Conclusions
In conclusion, the Random Forest model performed the best in predicting the median value of owner-occupied homes in the Boston Housing dataset. The model showed better performance than linear regression, Ridge Regression and Feature-selected models. Although I have selected the variables with the highest coefficient estimates for "medv" in the Feature-selected model, the results for the RMSE indicate that it may be more advantageous to utilize all available features, as they perform better with other models.  
Overall, our results demonstrate that machine learning can be an effective tool for predicting housing prices, and with further research and refinement, these models can be even more accurate

## References
The Boston Housing dataset can be found in the "MASS" package in R